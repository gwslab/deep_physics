{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "ead969f3039f046191c010f7ebfea3f38137df21110efa10ee7a069a02443296"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "import utils as u\n",
    "\n",
    "\n",
    "from torch.distributions import constraints \n",
    "import pyro.contrib.autoguide as autoguide\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, Trace_ELBO, config_enumerate, infer_discrete\n",
    "from pyro.optim import Adam, SGD\n",
    "from pyro import poutine\n",
    "import pyro"
   ]
  },
  {
   "source": [
    "We will first create the dataset for which we will construct the distribution. This dataset can be observed below. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_std = 0.3\n",
    "tangential_std = 0.05\n",
    "num_classes = 5\n",
    "num_per_class = 300\n",
    "rate = 0.2\n",
    "data = u.make_pinwheel_data(radial_std,tangential_std,num_classes,num_per_class,rate)\n",
    "\n",
    "#mean = -100 * torch.ones((num_per_class,2))\n",
    "#variance = 0.1 * torch.ones((num_per_class,2 ))\n",
    "#data = dist.Normal(mean,variance).sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.scatter(data[:,0],data[:,1])\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_neural_net_idenity(neural_net, low_lim=-10, up_lim=10, num_points=15, epochs=25000, lr=0.0001, frequency=200):\n",
    "    \"Sets neural network to idenity in a specified region\"\n",
    "    x = torch.linspace(low_lim, up_lim, num_points, requires_grad=True)\n",
    "    y = torch.linspace(low_lim, up_lim, num_points, requires_grad=True)\n",
    "    ac, bc = torch.meshgrid((x,y))\n",
    "    acf = ac.flatten().unsqueeze(-1)\n",
    "    bcf = bc.flatten().unsqueeze(-1)\n",
    "    data = torch.cat((acf,bcf), dim=1)\n",
    "    data = data.detach()[torch.randperm(len(data))]\n",
    "\n",
    "    opt = torch.optim.SGD(neural_net.parameters(), lr=lr)\n",
    "    loss = torch.nn.MSELoss()\n",
    "\n",
    "#    data = data[torch.randperm(len(data))].float()\n",
    "    train = int(len(data) * 0.8)\n",
    "    train_loader = torch.utils.data.DataLoader(data[:train],len(data[:train]),shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(data[train:],200)\n",
    "\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1): \n",
    "        for batch in train_loader: \n",
    "            opt.zero_grad()\n",
    "            res = neural_net(batch)[0]\n",
    "            out = loss(res, batch)\n",
    "            out.backward()\n",
    "            opt.step()\n",
    "\n",
    "            losses.append(out.item())\n",
    "\n",
    "        if epoch % frequency == 0:\n",
    "            print(\"[{}] Loss:{:.2f}\".format(epoch, losses[-1]))\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "class Decoder(nn.Module): \n",
    "    def __init__(self, z_dim, x_dim, hidden_dim): \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, x_dim)\n",
    "        self.fcad = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #Gives parameters for the cholesky deocomposition\n",
    "        self.fc22 = nn.Linear(hidden_dim, 1)\n",
    "        self.fc11 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "        self.x_dim = x_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "    def forward(self, z): \n",
    "        hidden = self.non_linearity(self.fc1(z))\n",
    "        hidden = self.non_linearity(self.fc11(hidden))\n",
    "        x_loc = self.fc21(hidden)\n",
    "\n",
    "        x_scale = torch.exp(self.fc22(hidden))\n",
    "        return x_loc , x_scale\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim, z_dim, hidden_dim): \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(x_dim,hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc11 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x): \n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        hidden = self.softplus(self.fc11(hidden))\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "\n",
    "        return z_loc, z_scale\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module): \n",
    "    def __init__(self, x_dim=2, z_dim=2, hidden_dim=2, K=3, use_cuda=False, init_lr=0.001, init_epochs=20000): \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(x_dim,z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, x_dim, hidden_dim)\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.x_dim = x_dim\n",
    "        self.K = K\n",
    "        set_neural_net_idenity(self.encoder, epochs=init_epochs, lr=init_lr)\n",
    "        set_neural_net_idenity(self.decoder, epochs=init_epochs, lr=init_lr)\n",
    "\n",
    "    @config_enumerate \n",
    "    def model(self, x):\n",
    "        pyro.module('decoder', self.decoder)\n",
    "\n",
    "        scale = pyro.param('scale', torch.tensor(1.), constraints.interval(0.2,10))\n",
    "        latent_scale = pyro.sample('latent_scale', dist.LogNormal(-1, 1.0))\n",
    "        weights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(self.K)))\n",
    "        with pyro.plate('components', self.K):\n",
    "            m_loc = x.new_zeros(torch.Size((self.K, self.z_dim)))\n",
    "            m_scale =  2 * x.new_ones(torch.Size((self.K, self.z_dim)))\n",
    "            locs = pyro.sample('locs',dist.Normal(m_loc, m_scale).to_event(1))\n",
    "\n",
    "        with pyro.plate('data', x.shape[0]):\n",
    "            assignment = pyro.sample('assigment', dist.Categorical(weights))\n",
    "            z = pyro.sample('latent', dist.Normal(locs[assignment],latent_scale).to_event(1))\n",
    "            loc_img, var_img = self.decoder(z)\n",
    "            var_img = torch.eye(x.shape[1]) * scale\n",
    "            out = pyro.sample('obs', dist.MultivariateNormal(loc_img, var_img), obs=x)\n",
    "\n",
    "\n",
    "    def guide(self, x):\n",
    "        pyro.module('encoder', self.encoder)\n",
    "        scale_mean = pyro.param('scale_mean', torch.ones(1))\n",
    "        scale_variance = pyro.param('scale_var', torch.ones(1), constraint=constraints.positive)\n",
    "        concentration = pyro.param('concentration', torch.ones(1), constraint=constraints.positive)\n",
    "\n",
    "        location_means = pyro.param('location_means', torch.ones(self.K, self.z_dim))\n",
    "        location_scale = pyro.param('location_scale', torch.ones(1), constraint=constraints.positive)\n",
    "\n",
    "        pyro.sample('weights', dist.Dirichlet(concentration * torch.ones(self.K)))\n",
    "        pyro.sample('latent_scale', dist.LogNormal(scale_mean, scale_variance))\n",
    "        with pyro.plate('components', self.K): \n",
    "            locs = pyro.sample('locs', dist.Normal(location_means, location_scale).to_event(1))\n",
    "\n",
    "\n",
    "\n",
    "        pyro.module('encoder', self.encoder)\n",
    "        with pyro.plate('data', x.shape[0]):\n",
    "            z_loc, z_scale = self.encoder(x)\n",
    "            out = pyro.sample('latent', dist.Normal(z_loc,z_scale).to_event(1))\n",
    "\n",
    "\n",
    "    def sample_latent(self,num_samples):\n",
    "\n",
    "        scale = pyro.param('scale')\n",
    "        scale_mean = pyro.param('scale_mean')\n",
    "        scale_variance = pyro.param('scale_var')\n",
    "        concentration = pyro.param('concentration')\n",
    "        location_means = pyro.param('location_means')\n",
    "\n",
    "        #I use the mean for the value of the scale \n",
    "        latent_scale = torch.exp(scale_mean  - scale_variance ** 2)\n",
    "        weights = pyro.sample('weights', dist.Dirichlet(concentration * torch.ones(self.K)))\n",
    "\n",
    "        with pyro.plate('data', num_samples):\n",
    "            assignment = pyro.sample('assigment', dist.Categorical(weights))\n",
    "            z = pyro.sample('latent', dist.Normal(location_means[assignment],latent_scale).to_event(1))\n",
    "            loc_img, var_img = self.decoder(z)\n",
    "            var_img = torch.eye(self.x_dim) * scale\n",
    "            res = pyro.sample('obser', dist.MultivariateNormal(loc_img, var_img))\n",
    "        return assignment, res\n",
    "\n",
    "    def sample_cluster(self, num_samples, location):\n",
    "        scale = pyro.param('scale')\n",
    "        scale_mean = pyro.param('scale_mean')\n",
    "        scale_variance = pyro.param('scale_var')\n",
    "        concentration = pyro.param('concentration')\n",
    "        location_means = pyro.param('location_means')\n",
    "\n",
    "        #I use the mean for the value of the scale \n",
    "        latent_scale = torch.exp(scale_mean  - scale_variance ** 2)\n",
    "        weights = pyro.sample('weights', dist.Dirichlet(concentration * torch.ones(self.K)))\n",
    "\n",
    "        with pyro.plate('data', num_samples):\n",
    "            z = pyro.sample('latent', dist.Normal(location_means[location],latent_scale).to_event(1))\n",
    "            loc_img, var_img = self.decoder(z)\n",
    "            var_img = torch.eye(self.x_dim) * scale\n",
    "            res = pyro.sample('obser', dist.MultivariateNormal(loc_img, var_img))\n",
    "        return res\n",
    "\n",
    "\n",
    "    def reconstruct(self, x): \n",
    "        mean, var = self.encoder(x)\n",
    "        z = dist.Normal(mean, var).sample()\n",
    "        mean, var = self.decoder(z)\n",
    "        var = torch.eye(self.x_dim) * pyro.param('scale')\n",
    "        res = dist.MultivariateNormal(mean, var).sample()\n",
    "        return res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data).float()\n",
    "train = int(len(data) * 0.8)\n",
    "train_loader = torch.utils.data.DataLoader(data[:train],len(data[:train]),shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(data[train:],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False): \n",
    "    epoch_loss = 0\n",
    "\n",
    "\n",
    "    for minibatch in train_loader: \n",
    "        if use_cuda:\n",
    "            minibatch = minibatch.cuda()\n",
    "        epoch_loss = svi.step(minibatch)\n",
    "\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "\n",
    "    total_epoch_loss_train = epoch_loss/normalizer_train\n",
    "    return total_epoch_loss_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "USE_CUDA = False\n",
    "K = 5\n",
    "NUM_EPOCHS = 20000\n",
    "TEST_FREQUENCY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "vae = VAE(hidden_dim=20, init_epochs=2000, K=K)\n",
    "\n",
    "adam_args = {'lr':LR}\n",
    "optimizer = torch.optim.Adam\n",
    "scheduler = pyro.optim.StepLR({ 'step_size' : 15000, 'gamma':0.01,'verbose':False, 'optimizer':optimizer,'optim_args':adam_args})\n",
    "\n",
    "guide = autoguide.AutoDiagonalNormal(vae.model)\n",
    "svi = SVI(vae.model, vae.guide, scheduler,loss=TraceEnum_ELBO(max_plate_nesting=1))\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(total_epoch_loss_train)\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "        scheduler.step()\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0: \n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((np.log(np.array(train_elbo))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real = data.detach().numpy()\n",
    "cluster = 4\n",
    "real = data\n",
    "sampled = vae.sample_cluster(10000, cluster)\n",
    "sampled = sampled.detach().numpy()\n",
    "plt.scatter(sampled[:,0],sampled[:,1],c='r', alpha =0.1)\n",
    "plt.scatter(real[:,0],real[:,1],c='b',alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = data \n",
    "ass, sampled = vae.sample_latent(10000)\n",
    "sampled = sampled.detach().numpy()\n",
    "plt.scatter(sampled[:,0],sampled[:,1],c=ass**2, alpha =0.1)\n",
    "plt.scatter(real[:,0],real[:,1],c='b',alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = vae.reconstruct(datat).detach().numpy()\n",
    "color = real[:,1]/np.abs(real[:,1])\n",
    "plt.scatter(reconstructed[:,0],reconstructed[:,1])\n",
    "plt.scatter(real[:,0],real[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to predict membership \n",
    "guide_trace = poutine.trace(vae.guide).get_trace(data)\n",
    "trained_model = poutine.replay(vae.model,trace=guide_trace)\n",
    "def classifier(data, temperature=0):\n",
    "    inferred_model = infer_discrete(trained_model, temperature=temperature,first_available_dim=-3)  # avoid conflict with data plate\n",
    "    trace = poutine.trace(inferred_model).get_trace(data)\n",
    "    return trace.nodes[\"assignment\"][\"value\"]\n",
    "\n",
    "print(classifier(data))\n"
   ]
  },
  {
   "source": [
    "# Second attempt using a full bayesian specification. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyro.param('location_means'))\n",
    "print(pyro.param('location_scale'))\n",
    "print(pyro.param('scale_var'))\n",
    "print(pyro.param('scale_mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}