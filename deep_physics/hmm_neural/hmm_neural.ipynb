{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from utils.utils import generate_linear_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyro stuff\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine \n",
    "from pyro.infer.autoguide import AutoDelta \n",
    "from pyro.infer import SVI, TraceEnum_ELBO, JitTraceEnum_ELBO, infer_discrete, config_enumerate\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.optim import Adam\n",
    "from pyro.util import ignore_jit_warnings\n"
   ]
  },
  {
   "source": [
    "# Data generation\n",
    "In the cells below we generate the data. \n",
    "It essentially consists of a ball going up and down in one dimension image. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pixels = generate_linear_data(num_balls=1, num_pixels=15, time =60*40, sf=10, max_period =5, noise_level=0.0)\n",
    "print(pixels)\n",
    "plt.imshow(pixels[:,:100], aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Before we proceed we need to do some slight preprocessing. \n",
    "Essentially I will take the data that I have above and I will divide it into smaller sequences. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 40\n",
    "elem_per_sequence = int(pixels.shape[-1]/num_sequences)\n",
    "sequences = []\n",
    "\n",
    "i = 0 \n",
    "#Repeat while we can take one more batch \n",
    "while (i + 1) * elem_per_sequence < pixels.shape[-1]: \n",
    "    start = i * elem_per_sequence\n",
    "    end = (i + 1) * elem_per_sequence\n",
    "    sequences.append(pixels[:,start:end])\n",
    "    i += 1\n",
    "\n",
    "# We just reshape or data to have [len,dim]\n",
    "sequences = torch.tensor(np.array(sequences))\n",
    "sequences = sequences.permute(0,2,1).float()\n",
    "sequences.dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "source": [
    "# Model definition\n",
    "We will define two models. First we will define a simple HMM with neural emission probabilities. Then we will define and autoregressive HMM with neural emission probabilities \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The first thing to do is to define the general parameters that our model will use, like the number of states of our markov chain. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of parameters\n",
    "\n",
    "states = 2\n",
    "batch_size = 10\n",
    "num_observations = pixels.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpale HMM model \n",
    "\n",
    "def model_0(sequences,num_states = 5, batch_size=None): \n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, length, data_dim = map(int, sequences.shape)\n",
    "\n",
    "    #Prior on the transition probabilities\n",
    "    probs_x = pyro.sample(\n",
    "        \"probs_x\", \n",
    "        dist.Dirichlet(0.9 * torch.eye(num_states) + 0.1).to_event(1)\n",
    "    )\n",
    "\n",
    "\n",
    "    probs_y = pyro.sample(\n",
    "        \"probs_y\", \n",
    "        dist.Beta(0.1, 0.9).expand([num_states, data_dim]).to_event(2)\n",
    "    )   \n",
    "\n",
    "    pixels_plate = pyro.plate('pixels',data_dim, dim=-1)\n",
    "    for i in pyro.plate('sequences', len(sequences), batch_size): \n",
    "        sequence = sequences[i]\n",
    "        x = 0\n",
    "\n",
    "        for t in pyro.markov(range(length)):\n",
    "            x = pyro.sample(\n",
    "                'x_{}_{}'.format(i,t), \n",
    "                dist.Categorical(probs_x[x]),\n",
    "                infer={'enumerate': 'parallel'}\n",
    "            )\n",
    "           \n",
    "            with pixels_plate:\n",
    "                pyro.sample(\n",
    "                    'y_{}_{}'.format(i, t),\n",
    "                    dist.Bernoulli(probs_y[x.squeeze(-1)]),\n",
    "                    obs = sequence[t]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use the second faster model and see if it works\n",
    "def model_2(sequences, num_states=2, batch_size=batch_size):\n",
    "    \n",
    "    num_sequences, length, data_dim = map(int, sequences.shape)\n",
    "\n",
    "\n",
    "    #Prior on the transition probabilities\n",
    "    probs_x = pyro.sample(\n",
    "        \"probs_x\", \n",
    "        dist.Dirichlet(0.9 * torch.eye(num_states) + 0.1).to_event(1)\n",
    "    )\n",
    "\n",
    "\n",
    "    probs_y = pyro.sample(\n",
    "        \"probs_y\", \n",
    "        dist.Beta(0.1, 0.9).expand([num_states, 2, data_dim]).to_event(3)\n",
    "    )   \n",
    "\n",
    "    pixels_plate = pyro.plate('pixels',data_dim, dim=-1)\n",
    "    with pyro.plate('sequences', num_sequences, batch_size, dim=-2) as batch: \n",
    "        x = 0\n",
    "        y = 0\n",
    "        for t in pyro.markov(range(length)):\n",
    "            x = pyro.sample(\n",
    "                'x_{}'.format(t), \n",
    "                dist.Categorical(probs_x[x]),\n",
    "                infer={'enumerate': 'parallel'}\n",
    "            ).long()\n",
    "           \n",
    "            with pixels_plate as tones:\n",
    "                y = pyro.sample(\n",
    "                    'y_{}'.format(t),\n",
    "                    dist.Bernoulli(probs_y[x, y, tones]),\n",
    "                    obs = sequences[batch,t]\n",
    "                    ).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an arHMM model. \n",
    "# I am ussing it as a way of having a threshold with which to compare our model\n",
    "# If our model is just as good as an arHMM model then there is no point in having something more complicated\n",
    "def model_1(sequences, num_states=2, batch_size=batch_size):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, length, data_dim = map(int, sequences.shape)\n",
    "\n",
    "\n",
    "    #Prior on the transition probabilities\n",
    "    probs_x = pyro.sample(\n",
    "        \"probs_x\", \n",
    "        dist.Dirichlet(0.9 * torch.eye(num_states) + 0.1).to_event(1)\n",
    "    )\n",
    "\n",
    "\n",
    "    probs_y = pyro.sample(\n",
    "        \"probs_y\", \n",
    "        dist.Beta(0.1, 0.9).expand([num_states, data_dim]).to_event(2)\n",
    "    )   \n",
    "\n",
    "    pixels_plate = pyro.plate('pixels',data_dim, dim=-1)\n",
    "    with pyro.plate('sequences', num_sequences, batch_size, dim=-2) as batch: \n",
    "        x = int(np.random.rand * num_states)\n",
    "        for t in pyro.markov(range(length)):\n",
    "            x = pyro.sample(\n",
    "                'x_{}_{}'.format(i,t), \n",
    "                dist.Categorical(probs_x[x]),\n",
    "                infer={'enumerate': 'parallel'}\n",
    "            )\n",
    "           \n",
    "            with pixels_plate:\n",
    "                pyro.sample(\n",
    "                    'y_{}_{}'.format(i, t),\n",
    "                    dist.Bernoulli(probs_y[x.squeeze(-1)]),\n",
    "                    obs = sequences[batch,t]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we put the code for a neural arhmm\n",
    "class PixelGenerator(nn.Module): \n",
    "    def __init__(self, args, data_dim):\n",
    "        \"\"\"\n",
    "        I will make args a dictionary so that I can pass the data\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.data_dim = data_dim \n",
    "        super().__init__()\n",
    "        self.x_to_hidden = nn.Linear(args['hidden_dim'], args['nn_dim'])\n",
    "        self.y_to_hidden = nn.Linear(args['nn_channels'] * data_dim, args['nn_dim'])\n",
    "        self.conv = nn.Conv1d(1, args['nn_channels'], 3, padding=1)\n",
    "        self.hidden_to_logits = nn.Linear(args['nn_dim'], data_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x , y): \n",
    "        x_onehot = y.new_zeros(x.shape[:-1] + (self.args['hidden_dim'],)).scatter_(-1, x, 1)\n",
    "        y_conv = self.relu(self.conv(y.reshape(-1,1, self.data_dim))).reshape(y.shape[:-1] + (-1,))\n",
    "        h = self.relu(self.x_to_hidden(x_onehot) + self.y_to_hidden(y_conv))\n",
    "        return self.hidden_to_logits(h)\n",
    "\n",
    "\n",
    "pixel_generator = None\n",
    "def model_5(sequences, num_states, args, batch_size=None):\n",
    "    num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "    global pixel_generator\n",
    "    if pixel_generator is None: \n",
    "        pixel_generator = PixelGenerator(args, data_dim)\n",
    "    \n",
    "    probs_x = pyro.sample('probs_x', dist.Dirichlet(0.9 * torch.eye(args['hidden_dim']) + 0.1).to_event(1))\n",
    "    \n",
    "    with pyro.plate('sequences', num_sequences, batch_size, dim=-2) as batch: \n",
    "        x = 0 \n",
    "        y = torch.zeros(data_dim)\n",
    "        for t in pyro.markov(range(max_length)):\n",
    "            x = pyro.sample(\n",
    "                \"x_{}\".format(t), dist.Categorical(probs_x[x]),\n",
    "                infer={'enumerate':'parallel'})\n",
    "\n",
    "            with pyro.plate('tones_{}'.format(t), data_dim, dim=-1):\n",
    "                y = pyro.sample(\n",
    "                        'y_{}'.format(t),\n",
    "                        dist.Bernoulli(logits=pixel_generator(x,y)),\n",
    "                        obs=sequences[batch,t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arguments for our neural networks\n",
    "\n",
    "args = {\n",
    "    'hidden_dim':2,\n",
    "    'nn_dim':48,\n",
    "    'nn_channels':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model_5\n",
    "guide = AutoDelta(poutine.block(\n",
    "    model, \n",
    "    expose_fn=lambda msg: msg['name'].startswith('probs_')\n",
    "))\n",
    "\n",
    "#this depends on whether we are using model zero or not\n",
    "first_available_dim = -3\n",
    "guide_trace = poutine.trace(guide).get_trace(\n",
    "    sequences, states, batch_size = batch_size, args= args\n",
    ")\n",
    "model_trace = poutine.trace(\n",
    "    poutine.replay(poutine.enum(model, first_available_dim), guide_trace)).get_trace(\n",
    "        sequences, batch_size=batch_size, num_states=states, args=args\n",
    "    )\n",
    "print(model_trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training args\n",
    "lr = 0.05\n",
    "num_steps = 500\n",
    "max_plate_nesting = 2\n",
    "batch_size = 10\n",
    "report_freq = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "optim = Adam({'lr': lr})\n",
    "elbo = TraceEnum_ELBO(\n",
    "    max_plate_nesting = max_plate_nesting)\n",
    "\n",
    "\n",
    "svi = SVI(model, guide, optim, elbo)\n",
    "\n",
    "loss = []\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "    loss.append(svi.step(sequences, num_states=states, batch_size=batch_size, args=args)/num_observations)\n",
    "    if step % report_freq == 0:\n",
    "        print(\"step: svi - step {}, loss {}\".format(step,loss[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_title = 'arhmm '\n",
    "plt.plot(-np.array(loss))\n",
    "#plt.title(model_title + str(states))\n",
    "#plt.savefig(\"elbo_\"+model+\"_\"+str(states)+\".png\",dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a funciton to check that our values make sense\n",
    "def ppc_vanilla_hmm(probs_x, probs_y, length_sample):\n",
    "    x = [0]\n",
    "    y=[]\n",
    "    for t in range(length_sample):\n",
    "        x.append(dist.Categorical(probs_x[x[-1]]).sample())\n",
    "        y.append(dist.Bernoulli(probs_y[x[-1]]).sample())\n",
    "    return x, y\n",
    "\n",
    "def ppc_arhmm(probs_x, probs_y, length_sample, dim_data): \n",
    "    x = [int(np.random.rand()*states)]\n",
    "    y = [torch.zeros(dim_data).long()]\n",
    "    pixels = torch.tensor(list(range(dim_data)))\n",
    "    print(x)\n",
    "    for t in range(length_sample):\n",
    "        x.append(dist.Categorical(probs_x[x[-1]]).sample())\n",
    "        y.append(dist.Bernoulli(probs_y[x[-1], y[-1], pixels]).sample().long())\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pyro.get_param_store()\n",
    "probs_x = values['AutoDelta.probs_x']\n",
    "probs_y = values['AutoDelta.probs_y']\n",
    "x, y = ppc_arhmm(probs_x, probs_y, 1000, 15)\n",
    "y_np = np.array([a.numpy() for a in y])\n",
    "x_np = np.array([a.numpy() for a in x[1:]])\n",
    "plt.imshow(y_np[:100].T, aspect='auto')\n",
    "plt.show()\n",
    "#plt.savefig('simulated_data_arhmm_10_states.png', dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.expand_dims(x_np,-1).T, aspect='auto')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@infer_discrete(first_available_dim=-3, temperature=0)\n",
    "@config_enumerate\n",
    "def viterbi_decoder_arhmm(sequence, probs_x, probs_y, num_states=2):\n",
    "    length, data_dim = map(int, sequence.shape)\n",
    "    #Prior on the transition probabilities\n",
    "    pixels_plate = pyro.plate('pixels',data_dim, dim=-1)\n",
    "    x_list = [0]\n",
    "    x= 0\n",
    "    y = 0\n",
    "    for t in pyro.markov(range(length)):\n",
    "        x = pyro.sample(\n",
    "            'x_{}'.format(t), \n",
    "            dist.Categorical(probs_x[x]),\n",
    "            ).long()\n",
    "        x_list.append(x)\n",
    "           \n",
    "        with pixels_plate as tones:\n",
    "            y = pyro.sample(\n",
    "                'y_{}'.format(t),\n",
    "                dist.Bernoulli(probs_y[x, y, tones]),\n",
    "                 obs = sequence[t]\n",
    "                                  ).long()\n",
    "\n",
    "    return x_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered_states_0 = [a.numpy() for a in viterbi_decoder_arhmm(sequences[0], probs_x, probs_y)[1:30]]\n",
    "plt.imshow(np.expand_dims(infered_states_0,-1).T, aspect='auto')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sequences[0][1:100].T, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand()*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}